{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"13pZXEHJ8SO8icQV_8Qbh1PaMt3zrT6Oy","timestamp":1663065966607}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xF3xcTZkX0XC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb1b491c-6a4a-4c85-ea98-45d0a32615c4","executionInfo":{"status":"ok","timestamp":1663672150722,"user_tz":-300,"elapsed":2454,"user":{"displayName":"Ahtsham Zafar","userId":"07516761739890635166"}}},"source":["import torch\n","from torch.utils.data import Dataset\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","import torch.nn.functional as F\n","import matplotlib.pylab as plt\n","import numpy as np\n","import torchvision\n","torch.manual_seed(2)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_on_gpu = torch.cuda.is_available()\n","if not train_on_gpu:\n","    print('CUDA is not available')\n","else:\n","    print('CUDA is available!')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA is available!\n"]}]},{"cell_type":"code","metadata":{"id":"PllHdwT1X4kq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3c1ccf4f-e86e-42fc-cf6a-6e83ddec0c7c","executionInfo":{"status":"ok","timestamp":1641106856712,"user_tz":-300,"elapsed":4625,"user":{"displayName":"Ahtsham Zafar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglJdFTNAx8xfuHVrEFJFCHSQRuL4s2neaqUkcrhg=s64","userId":"07516761739890635166"}}},"source":["from google.colab import drive, files\n","drive.mount('/content/drive/')\n","trainpath = \"/content/drive/MyDrive/AV Dataset/TRAIN\"\n","validationpath = \"/content/drive/MyDrive/AV Dataset/TEST\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"5BFYdcZWX69I"},"source":["#Test Transform and data loader\n","transform1 = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\n","train_dataset = dsets.ImageFolder(root=trainpath, transform=transform1)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=32, shuffle=True)\n","\n","#validation transform and dataloader\n","validation_transform = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\n","test_dataset = dsets.ImageFolder(root=validationpath, transform=validation_transform)\n","validation_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=32, shuffle=False)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_ihyUQzX8r_"},"source":["num_epochs = 3\n","num_classes = 3\n","batch_size = 32\n","learning_rate = 0.01\n","print('there are {}  images in training set.'.format(len(train_dataset)))\n","print('there are {}  images in test set.'.format(len(test_dataset)))\n","print('there are {}  images in training set.'.format(len(train_loader)))\n","print('there are {}  images in training loader.'.format(len(validation_loader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BbhTAbn9X-SZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"13514ac9-deed-462f-bac6-5f4ec179f07f","executionInfo":{"status":"ok","timestamp":1663672196168,"user_tz":-300,"elapsed":723,"user":{"displayName":"Ahtsham Zafar","userId":"07516761739890635166"}}},"source":["class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.conv_layer1 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3)\n","        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n","        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        \n","        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n","        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n","        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","\n","        self.conv_layer5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n","        self.conv_layer6= nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3)\n","       \n","        \n","        self.fc1 = nn.Linear(671616, 10)\n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(10, 3)\n","\n","    def forward(self, x):\n","        out = self.conv_layer1(x)\n","        out = self.conv_layer2(out)\n","        out = self.max_pool1(out)\n","        \n","        out = self.conv_layer3(out)\n","        out = self.conv_layer4(out)\n","        out = self.max_pool2(out)\n","\n","        out = self.conv_layer5(out)\n","        out = self.conv_layer6(out)\n","       \n","\n","        out = out.reshape(out.size(0), -1)\n","        \n","        out = self.fc1(out)\n","        out = self.relu1(out)\n","        out = self.fc2(out)\n","        return out\n","model1=ConvNet()\n","print(model1)     "],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["ConvNet(\n","  (conv_layer1): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (conv_layer2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv_layer3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (conv_layer4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv_layer5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n","  (conv_layer6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=671616, out_features=10, bias=True)\n","  (relu1): ReLU()\n","  (fc2): Linear(in_features=10, out_features=3, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["from torchsummary import summary\n","\n","summary(model1.cuda(), (8, 240, 426))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0rvGvVESWh7","executionInfo":{"status":"ok","timestamp":1663672199959,"user_tz":-300,"elapsed":651,"user":{"displayName":"Ahtsham Zafar","userId":"07516761739890635166"}},"outputId":"d2b56175-d4fa-47f8-a681-2cba5621682f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 238, 424]           2,336\n","            Conv2d-2         [-1, 32, 236, 422]           9,248\n","         MaxPool2d-3         [-1, 32, 118, 211]               0\n","            Conv2d-4         [-1, 64, 116, 209]          18,496\n","            Conv2d-5         [-1, 64, 114, 207]          36,928\n","         MaxPool2d-6          [-1, 64, 57, 103]               0\n","            Conv2d-7         [-1, 128, 55, 101]          73,856\n","            Conv2d-8          [-1, 128, 53, 99]         147,584\n","            Linear-9                   [-1, 10]       6,716,170\n","             ReLU-10                   [-1, 10]               0\n","           Linear-11                    [-1, 3]              33\n","================================================================\n","Total params: 7,004,651\n","Trainable params: 7,004,651\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 3.12\n","Forward/backward pass size (MB): 91.81\n","Params size (MB): 26.72\n","Estimated Total Size (MB): 121.65\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"id":"xNSpgZ1JYAcn"},"source":["model = ConvNet().to (device)\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss().to (device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bypjnL82YEdy"},"source":["# Train the model\n","print(len(train_loader))\n","total_step = len(train_loader)\n","loss_list = []\n","acc_list = []\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # Run the forward pass\n","        images=images.to (device)\n","        labels=labels.to (device)\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss_list.append(loss.item())\n","\n","        # Backprop and perform Adam optimisation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Track the accuracy\n","        total = labels.size(0)\n","        _, predicted = torch.max(outputs.data, 1)\n","        correct = (predicted == labels).sum().item()\n","        acc_list.append(correct / total)\n","        print(\"Training\")\n","        #if (i + 1) % 100 == 0:\n","        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),(correct / total) * 100))          \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4P34lUM2lOsw"},"source":["model = ConvNet()\n","model.load_state_dict(torch.load(r'/content/drive/My Drive/cnn_covid19_asad_asp.pth'))\n","model = model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9c83_w63BxxQ"},"source":["model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in validation_loader:\n","        images=images.to (device)\n","        labels=labels.to (device)\n","        outputs = model(images).to(device)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the model on the  test images: {} %'.format((correct / total) * 100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eY_Neh1WH7K9"},"source":["from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yeT94i-6Hwt3"},"source":["from sklearn.metrics import confusion_matrix\n","\n","nb_classes = 3\n","\n","# Initialize the prediction and label lists(tensors)\n","predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n","lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n","\n","with torch.no_grad():\n","    for i, (inputs, classes) in enumerate(validation_loader):\n","        inputs = inputs.to(device)\n","        classes = classes.to(device)\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","\n","        # Append batch prediction results\n","        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n","        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n","\n","# Confusion matrix\n","conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n","print(conf_mat)\n","sns.heatmap(conf_mat, annot=True, cbar=False)\n","\n","ax= plt.subplot()\n","# Per-class accuracy\n","#class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n","#print(class_accuracy)\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in validation_loader:\n","        images=images.to (device)\n","        labels=labels.to (device)\n","        outputs = model(images).to(device)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the model on the  test images: {} %'.format((correct / total) * 100))\n","    #sns.heatmap(cm, cmap=\"YlGnBu\",annot=True, ax = ax,cbar=False); #annot=True to annotate cells\n","\n","\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n","ax.set_title('Confusion Matrix'); \n","ax.xaxis.set_ticklabels(['Car', 'Human','No Object']); ax.yaxis.set_ticklabels(['Car', 'Human','No Object']);"],"execution_count":null,"outputs":[]}]}